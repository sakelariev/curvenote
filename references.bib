@UNPUBLISHED{Deffner2021-vp,
  title    = "A Causal Framework for {Cross-Cultural} Generalizability",
  author   = "Deffner, Dominik and Rohrer, Julia M and McElreath, Richard",
  abstract = "Behavioral researchers increasingly recognize the need for more
              diverse samples that capture the breadth of human experience.
              Current attempts to establish generalizability across populations
              focus on threats to validity, constraints on generalization and
              the accumulation of large cross-cultural datasets. But for
              continued progress, we also require a framework that lets us
              determine which inferences can be drawn and how to make
              informative cross-cultural comparisons. We describe a generative
              causal modeling framework and outline simple graphical criteria
              to derive analytic strategies and implied generalizations. Using
              both simulated and real data, we demonstrate how to project and
              compare estimates across populations. We conclude with a
              discussion of how a formal framework for generalizability can
              assist researchers in designing more informative cross-cultural
              studies and thus provides a more solid foundation for cumulative
              and generalizable behavioral research.",
  month    =  sep,
  year     =  2021,
  url      = "psyarxiv.com/fqukp",
  annote   = "Also good for market research probably",
  file     = "All Papers/D/Deffner et al. 2021 - A Causal Framework for Cross-Cultural Generalizability.pdf",
  keywords = "causal inference; Cross-cultural research; generalizability;
              postratification; WEIRD samples problem;Methods;Market
              Research;Cultural Evolution;Causal Inference;Bibliography",
  doi      = "10.31234/osf.io/fqukp"
}

@BOOK{Pearl2016-ei,
  title     = "Causal Inference in Statistics",
  author    = "Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P",
  abstract  = "Many of the concepts and terminology surrounding modern causal
               inference can be quite intimidating to the novice. Judea Pearl
               presents a book ideal for beginners in statistics, providing a
               comprehensive introduction to the field of causality. Examples
               from classical statistics are presented throughout to
               demonstrate the need for causality in resolving decision-making
               dilemmas posed by data. Causal methods are also compared to
               traditional statistical methods, whilst questions are provided
               at the end of each section to aid student learning.",
  publisher = "John Wiley \& Sons",
  month     =  "4~" # mar,
  year      =  2016,
  url       = "https://www.wiley.com/en-us/Causal+Inference+in+Statistics:+A+Primer-p-9781119186847",
  address   = "Nashville, TN",
  keywords  = "Causal Inference;Bibliography",
  language  = "en",
  isbn      = "9781119186847"
}

@ARTICLE{Brodersen2015-rp,
  title         = "Inferring causal impact using Bayesian structural
                   time-series models",
  author        = "Brodersen, Kay H and Gallusser, Fabian and Koehler, Jim and
                   Remy, Nicolas and Scott, Steven L",
  abstract      = "An important problem in econometrics and marketing is to
                   infer the causal impact that a designed market intervention
                   has exerted on an outcome metric over time. This paper
                   proposes to infer causal impact on the basis of a
                   diffusion-regression state-space model that predicts the
                   counterfactual market response in a synthetic control that
                   would have occurred had no intervention taken place. In
                   contrast to classical difference-in-differences schemes,
                   state-space models make it possible to (i) infer the
                   temporal evolution of attributable impact, (ii) incorporate
                   empirical priors on the parameters in a fully Bayesian
                   treatment, and (iii) flexibly accommodate multiple sources
                   of variation, including local trends, seasonality and the
                   time-varying influence of contemporaneous covariates. Using
                   a Markov chain Monte Carlo algorithm for posterior
                   inference, we illustrate the statistical properties of our
                   approach on simulated data. We then demonstrate its
                   practical utility by estimating the causal effect of an
                   online advertising campaign on search-related site visits.
                   We discuss the strengths and limitations of state-space
                   models in enabling causal attribution in those settings
                   where a randomised experiment is unavailable. The
                   CausalImpact R package provides an implementation of our
                   approach.",
  month         =  "1~" # jun,
  year          =  2015,
  url           = "http://arxiv.org/abs/1506.00356",
  file          = "All Papers/B/Brodersen et al. 2015 - Inferring causal impact using Bayesian structural time-series models.pdf",
  keywords      = "Work-related;Causal Inference",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1506.00356",
  primaryClass  = "stat.AP",
  arxivid       = "1506.00356"
}

@UNPUBLISHED{Smaldino2020-iw,
  title    = "How to build A strong theoretical foundation",
  author   = "Smaldino, Paul E",
  abstract = "It is our theories that shape how we see the world and the
              questions we ask of it. Fried argues that psychological science
              is plagued by weak theories, and that there is a real need for
              building and testing strong theories that include formal models.
              I agree. Fried calls for better training in the construction of
              formal theory, and I enthusiastically agree with this as well.
              However, I am concerned that the road to establish such a
              training in program will be long and hard. Fried ends his piece
              with tentative optimism, but little in the way of concrete
              proposals. Here I'll outline what I think some of the necessary
              changes are and why implementing them will be challenging. I'll
              conclude with some thoughts on how to overcome those challenges.
              Constructing good strong theories requires integration of the
              skills currently possessed by psychological scientists with (1)
              increased interdisciplinarity, (2) increased technical prowess,
              and (3) increased philosophical scrutiny.",
  journal  = "PsyArXiv",
  month    =  "29~" # jul,
  year     =  2020,
  url      = "https://osf.io/khrdx",
  file     = "All Papers/S/Smaldino 2020 - How to build A strong theoretical foundation.pdf",
  keywords = "Read;Methods;Causal Inference;Main",
  doi      = "10.31234/osf.io/khrdx"
}

@ARTICLE{Grosz2020-dv,
  title       = "The Taboo Against Explicit Causal Inference in Nonexperimental
                 Psychology",
  author      = "Grosz, Michael P and Rohrer, Julia M and Thoemmes, Felix",
  affiliation = "Department of Psychology, University of M{\"u}nster.
                 International Max Planck Research School on the Life Course,
                 Max Planck Institute for Human Development. Department of
                 Psychology, University of Leipzig. Department of Human
                 Development, Cornell University.",
  abstract    = "Causal inference is a central goal of research. However, most
                 psychologists refrain from explicitly addressing causal
                 research questions and avoid drawing causal inference on the
                 basis of nonexperimental evidence. We argue that this taboo
                 against causal inference in nonexperimental psychology impairs
                 study design and data analysis, holds back cumulative
                 research, leads to a disconnect between original findings and
                 how they are interpreted in subsequent work, and limits the
                 relevance of nonexperimental psychology for policymaking. At
                 the same time, the taboo does not prevent researchers from
                 interpreting findings as causal effects-the inference is
                 simply made implicitly, and assumptions remain unarticulated.
                 Thus, we recommend that nonexperimental psychologists begin to
                 talk openly about causal assumptions and causal effects. Only
                 then can researchers take advantage of recent methodological
                 advances in causal reasoning and analysis and develop a solid
                 understanding of the underlying causal mechanisms that can
                 inform future research, theory, and policymakers.",
  journal     = "Perspectives on psychological science: a journal of the
                 Association for Psychological Science",
  volume      =  15,
  number      =  5,
  pages       = "1243--1255",
  month       =  sep,
  year        =  2020,
  url         = "http://dx.doi.org/10.1177/1745691620921521",
  file        = "All Papers/G/Grosz et al. 2020 - The Taboo Against Explicit Causal Inference in Nonexperimental Psychology.pdf",
  keywords    = "causal inference; instrumental-variable estimation;
                 nonexperimental; observational studies;Causal Inference",
  language    = "en",
  issn        = "1745-6916, 1745-6924",
  pmid        = "32727292",
  doi         = "10.1177/1745691620921521",
  pmc         = "PMC7472833"
}

@ARTICLE{Van_de_Schoot2021-fx,
  title     = "Bayesian statistics and modelling",
  author    = "van de Schoot, Rens and Depaoli, Sarah and King, Ruth and
               Kramer, Bianca and M{\"a}rtens, Kaspar and Tadesse, Mahlet G and
               Vannucci, Marina and Gelman, Andrew and Veen, Duco and
               Willemsen, Joukje and Yau, Christopher",
  abstract  = "Bayesian statistics is an approach to data analysis based on
               Bayes' theorem, where available knowledge about parameters in a
               statistical model is updated with the information in observed
               data. The background knowledge is expressed as a prior
               distribution and combined with observational data in the form of
               a likelihood function to determine the posterior distribution.
               The posterior can also be used for making predictions about
               future events. This Primer describes the stages involved in
               Bayesian analysis, from specifying the prior and data models to
               deriving inference, model checking and refinement. We discuss
               the importance of prior and posterior predictive checking,
               selecting a proper technique for sampling from a posterior
               distribution, variational inference and variable selection.
               Examples of successful applications of Bayesian analysis across
               various research fields are provided, including in social
               sciences, ecology, genetics, medicine and more. We propose
               strategies for reproducibility and reporting standards,
               outlining an updated WAMBS (when to Worry and how to Avoid the
               Misuse of Bayesian Statistics) checklist. Finally, we outline
               the impact of Bayesian analysis on artificial intelligence, a
               major goal in the next decade.",
  journal   = "Nature Reviews Methods Primers",
  publisher = "Springer Science and Business Media LLC",
  volume    =  1,
  number    =  1,
  pages     = "1",
  month     =  "14~" # dec,
  year      =  2021,
  url       = "https://www.nature.com/articles/s43586-020-00001-2",
  file      = "All Papers/V/van de Schoot et al. 2021 - Bayesian statistics and modelling.pdf",
  keywords  = "Causal Inference",
  language  = "en",
  issn      = "2662-8449",
  doi       = "10.1038/s43586-020-00001-2"
}

@ARTICLE{Rohrer2018-zs,
  title       = "Thinking clearly about correlations and causation: Graphical
                 causal models for observational data",
  author      = "Rohrer, Julia M",
  affiliation = "International Max Planck Research School on the Life Course,
                 Max Planck Institute for Human Development, Berlin, Germany;
                 Department of Psychology, University of Leipzig; German
                 Institute for Economic Research, Berlin, Germany",
  abstract    = "Correlation does not imply causation; but often, observational
                 data are the only option, even though the research question at
                 hand involves causality. This article discusses causal
                 inference based on observational data, introducing readers to
                 graphical causal models that can provide a powerful tool for
                 thinking more clearly about the interrelations between
                 variables. Topics covered include the rationale behind the
                 statistical control of third variables, common procedures for
                 statistical control, and what can go wrong during their
                 implementation. Certain types of third variables---colliders
                 and mediators---should not be controlled for because that can
                 actually move the estimate of an association away from the
                 value of the causal effect of interest. More subtle variations
                 of such harmful control include using unrepresentative
                 samples, which can undermine the validity of causal
                 conclusions, and statistically controlling for mediators.
                 Drawing valid causal inferences on the basis of observational
                 data is not a mechanistic procedure but rather always depends
                 on assumptions that require domain knowledge and that can be
                 more or less plausible. However, this caveat holds not only
                 for research based on observational data, but for all
                 empirical research endeavors.",
  journal     = "Advances in methods and practices in psychological science",
  publisher   = "SAGE Publications",
  volume      =  1,
  number      =  1,
  pages       = "27--42",
  month       =  mar,
  year        =  2018,
  url         = "http://dx.doi.org/10.1177/2515245917745629",
  file        = "All Papers/R/Rohrer 2018 - Thinking clearly about correlations and causation - Graphical causal models for observational data.pdf",
  keywords    = "Causal Inference",
  language    = "en",
  issn        = "2515-2459, 2515-2467",
  doi         = "10.1177/2515245917745629"
}

@ARTICLE{Smaldino2016-gz,
  title       = "The natural selection of bad science",
  author      = "Smaldino, Paul E and McElreath, Richard",
  affiliation = "Cognitive and Information Sciences , University of California
                 , Merced, CA 95343, USA. Department of Human Behavior ,
                 Ecology, and Culture, Max Planck Institute for Evolutionary
                 Anthropology , Leipzig, Germany.",
  abstract    = "Poor research design and data analysis encourage
                 false-positive findings. Such poor methods persist despite
                 perennial calls for improvement, suggesting that they result
                 from something more than just misunderstanding. The
                 persistence of poor methods results partly from incentives
                 that favour them, leading to the natural selection of bad
                 science. This dynamic requires no conscious strategizing-no
                 deliberate cheating nor loafing-by scientists, only that
                 publication is a principal factor for career advancement. Some
                 normative methods of analysis have almost certainly been
                 selected to further publication instead of discovery. In order
                 to improve the culture of science, a shift must be made away
                 from correcting misunderstandings and towards rewarding
                 understanding. We support this argument with empirical
                 evidence and computational modelling. We first present a
                 60-year meta-analysis of statistical power in the behavioural
                 sciences and show that power has not improved despite repeated
                 demonstrations of the necessity of increasing power. To
                 demonstrate the logical consequences of structural incentives,
                 we then present a dynamic model of scientific communities in
                 which competing laboratories investigate novel or previously
                 published hypotheses using culturally transmitted research
                 methods. As in the real world, successful labs produce more
                 'progeny,' such that their methods are more often copied and
                 their students are more likely to start labs of their own.
                 Selection for high output leads to poorer methods and
                 increasingly high false discovery rates. We additionally show
                 that replication slows but does not stop the process of
                 methodological deterioration. Improving the quality of
                 research requires change at the institutional level.",
  journal     = "Royal Society open science",
  volume      =  3,
  number      =  9,
  pages       = "160384",
  month       =  sep,
  year        =  2016,
  url         = "http://dx.doi.org/10.1098/rsos.160384",
  file        = "All Papers/S/Smaldino and McElreath 2016 - The natural selection of bad science.pdf",
  keywords    = "Campbell's Law; cultural evolution; incentives; metascience;
                 replication; statistical power;Read;Causal Inference",
  language    = "en",
  issn        = "2054-5703",
  pmid        = "27703703",
  doi         = "10.1098/rsos.160384",
  pmc         = "PMC5043322"
}

@UNPUBLISHED{Rohrer2021-hu,
  title    = "The only thing that can stop bad causal inference is good causal
              inference",
  author   = "Rohrer, Julia M and Schmukle, Stefan C and McElreath, Richard",
  abstract = "In psychology, causal inference---both the transport from lab
              estimates to the real world and estimation on the basis of
              observational data---is often pursued in a casual manner.
              Underlying assumptions remain unarticulated; potential pitfalls
              are compiled in post-hoc lists of flaws. The field should move on
              to coherent frameworks of causal inference and generalizability
              that have been developed elsewhere.",
  month    =  "6~" # may,
  year     =  2021,
  url      = "psyarxiv.com/mz5jx",
  file     = "All Papers/R/Rohrer et al. 2021 - The only thing that can stop bad causal inference is good causal inference.pdf",
  keywords = "To Read;Causal Inference;Bibliography;Main",
  doi      = "10.31234/osf.io/mz5jx"
}
